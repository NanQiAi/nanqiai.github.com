<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<title>【2】LLaMa论文阅读笔记 - 机器学习论文阅读笔记</title>
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="generator" content="mkdocs-1.6.1, mkdocs-gitbook-1.0.7">
<meta name="author" content="Desheng Wang">
<link rel="shortcut icon" href="../images/favicon.ico" type="image/x-icon">
<meta name="HandheldFriendly" content="true"/>
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta rel="next" href="" />
<link href="../css/style.min.css" rel="stylesheet"> 
</head>

<body>
<div class="book">
<div class="book-summary">

<nav role="navigation">
<ul class="summary">
<li>
<a href=".." target="_blank" class="custom-link">机器学习论文阅读笔记</a>
</li>
<li class="divider"></li>
<li class="header">三、大语言模型篇</li>

<li>
<a href="../Language_Model%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="">【1】Language Model语言模型</a>
</li>

<li>
<a href="./" class="active">【2】LLaMa论文阅读笔记</a>
</li>

<li class="header">二、多模态篇</li>

<li>
<a href="../BLIP%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="">【1】BLIP论文阅读笔记</a>
</li>

<li>
<a href="../Tag2Text%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="">【2】Tag2Text论文阅读笔记</a>
</li>

<li>
<a href="../Recognize_Anything%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="">【3】Recognize Anything论文阅读笔记</a>
</li>

<li class="header">一、工具篇</li>

<li>
<a href="../mkdocs%E5%B8%AE%E5%8A%A9/" class="">【1】mkdocs帮助</a>
</li>

<li>
<a href=".." class="">【2】深度学习工具</a>
</li>

<li>
<a href="../%E5%9B%A2%E9%98%9F%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/" class="">【3】团队管理笔记</a>
</li>

<li class="divider"></li>


<li>2023, Desheng Wang</li>


<li><a href="http://www.mkdocs.org">
Published with MkDocs
</a></li>

<li><a href="https://github.com/GitbookIO/theme-default">
Theme by GitBook
</a></li>
</ul>

</nav>

</div> <!-- end of book-summary -->

<div class="book-body">
<div class="body-inner">
<div class="book-header" role="navigation">

<!-- Title -->
<h1>
<i class="fa fa-circle-o-notch fa-spin"></i>
<a href="." ></a>
</h1>

</div> <!-- end of book-header -->

<div class="page-wrapper" tabindex="-1" role="main">
<div class="page-inner">

<section class="normal markdown-section">



<h1 id="llama">LLaMA<a class="headerlink" href="#llama" title="Permanent link">&para;</a></h1>
<ul>
<li>Paper：《LLaMA: Open and Efficient Foundation Language Models》</li>
<li>inference budget在规模地serving语言模型时变得至关重要，首选的模型不是训练速度最快的，而是推理速度最快的，尽管训练一个达到一定性能水平的大模型可能更便宜（指的训练成本），但训练时间较长的小模型最终会在推理中更便宜。</li>
<li>尽管《Training compute-optimal large language models》建议在200B的token上训练一个10B的模型，但《LLaMA: Open and Efficient Foundation Language Models》的作者发现，即使在1T的token之后，7B的模型的性能也会继续提高。</li>
<li>For a given compute budget, the best performances are not achieved by the largest models, but by smaller models trained on more data. 说明小规模的模型喂入更多的数据，可能会带来模型效果的提升。前提是小规模的模型也不是很小，可能是7B-65B左右，模型可以容纳或者拟合很多的数据量，但是小模型训练的时间更加长。</li>
<li>数据集：公开渠道的数据分布 <img alt="" src="../pic/llama/LLaMA_data.png" /></li>
</ul>


</section>

</div> <!-- end of page-inner -->
</div> <!-- end of page-wrapper -->

</div> <!-- end of body-inner -->

</div> <!-- end of book-body -->
<script src="../js/main.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script src="../js/gitbook.min.js"></script>
<script src="../js/theme.min.js"></script>
</body>
</html>