<!DOCTYPE html>
<html class="writer-html5" lang="zh_CN" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="author" content="Desheng Wang" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>【1】BLIP论文阅读笔记 - 机器学习论文阅读笔记</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u30101\u3011BLIP\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0";
        var mkdocs_page_input_path = "BLIP\u8bba\u6587\u9605\u8bfb\u7b14\u8bb0.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/yaml.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> 机器学习论文阅读笔记
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">三、大语言模型篇</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../Language_Model%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">【1】Language Model语言模型</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../LLaMa%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">【2】LLaMa论文阅读笔记</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">二、多模态篇</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">【1】BLIP论文阅读笔记</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#1-introduction">1. Introduction</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#2-related-work">2. Related Work</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#21-vision-language-pre-training">2.1 Vision-language Pre-training</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#22-knowledge-distillation">2.2 Knowledge Distillation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#23-data-augmentation">2.3 Data Augmentation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#3-method">3. Method</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#31-model-architecture">3.1 Model Architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#32-pre-training-objectives">3.2 Pre-training Objectives</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#33-capfilt">3.3 CapFilt</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#4-experiments-and-discussions">4. Experiments and Discussions</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#6-additional-ablation-study">6. Additional Ablation Study</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Tag2Text%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">【2】Tag2Text论文阅读笔记</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../Recognize_Anything%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">【3】Recognize Anything论文阅读笔记</a>
                  </li>
              </ul>
              <p class="caption"><span class="caption-text">一、工具篇</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../mkdocs%E5%B8%AE%E5%8A%A9/">【1】mkdocs帮助</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="..">【2】深度学习工具</a>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">机器学习论文阅读笔记</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">二、多模态篇</li>
      <li class="breadcrumb-item active">【1】BLIP论文阅读笔记</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="blip">BLIP<a class="headerlink" href="#blip" title="Permanent link">&para;</a></h1>
<ul>
<li>Paper：《BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation》</li>
</ul>
<h2 id="1-introduction">1. Introduction<a class="headerlink" href="#1-introduction" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>当前模型的缺陷：</p>
<ul>
<li>模型视角：大多数据模型采用的是encoder-based model 或者 encoder-decoder model，但是encoder-decoder并没有成功解决image-text retrieval task，所以encoder-decoder model解决文本生成任务不够简单有效。</li>
<li>数据视角：当前SOAT的模型CLIP和ALBEF，SimVLM都在是网上公开获得的数据，尽管随着数据集规模加大，效果有提升，但是有噪声的文本数据对vision-language learning效果是次优的。</li>
</ul>
</li>
<li>
<p>BLIP模型的contributions：</p>
<ul>
<li>一种新高效预训练和弹性迁移学习的模型架构：Multimodal mixture of Encoder-Decoder (MED)。单模态encoder、image-grounded text encoder、image-grounded text decoder都支持。联合预训练三类Vision-language objectives: image- text contrastive learning, image-text matching, and image- conditioned language modeling。</li>
<li>Captioning and Filtering(CapFilt)：一种从噪声数据学习并扩展数据集方法。<strong>capitoner</strong>：从给定的网页图片中产生合成的caption，<strong>filter</strong>：从原始web文本数据和合成数据中剔除噪声caption。</li>
</ul>
</li>
<li>
<p>2个key observations：</p>
<ul>
<li><strong>captioner</strong>和<strong>filter</strong>合并一起工作，利用bootstrapping the captions在各种下游任务都取得较好的效果提升。</li>
<li>BLIP在一些列Vision-language task都取得SOAT的效果，包括image-text retrieval/image captioning/visual question answering/visual reasoning/visual dialog。同时把模型迁移至video-language task在Zero-shot方面也取得了SOAT的效果，包括text-to-video retrival和Video QA。</li>
</ul>
</li>
</ul>
<h2 id="2-related-work">2. Related Work<a class="headerlink" href="#2-related-work" title="Permanent link">&para;</a></h2>
<h3 id="21-vision-language-pre-training">2.1 Vision-language Pre-training<a class="headerlink" href="#21-vision-language-pre-training" title="Permanent link">&para;</a></h3>
<ul>
<li>Vision-language pre-training (VLP)目标：improve performance of downstream vision and language tasks by pre-training the model on large-scale image-text pairs。</li>
<li>最大的挑战是兼顾理解类任务（image-text检索）和生成类任务（image captioning）。The biggest challenge is to design model architectures that can perform both understanding-based tasks (e.g. image-text retrieval) and generation-based tasks (e.g. image captioning)，无论是encoder-based还是decoder-based模型都不擅长这两类任务，且单一统一的encoder-decoder限制了模型的性能。</li>
</ul>
<h3 id="22-knowledge-distillation">2.2 Knowledge Distillation<a class="headerlink" href="#22-knowledge-distillation" title="Permanent link">&para;</a></h3>
<ul>
<li>知识蒸馏（KD，Knowledge Distillation）：CapFilt：可以看做是一种在VLP环境（context）中更加高效方式的KD，可以从语义丰富的合成caption中蒸馏知识，且过滤其中噪音caption。看来，CapFilt是一种知识蒸馏方式实现的。</li>
</ul>
<h3 id="23-data-augmentation">2.3 Data Augmentation<a class="headerlink" href="#23-data-augmentation" title="Permanent link">&para;</a></h3>
<ul>
<li>DA（数据增强），对language task来说，DA不够简单有效。以往的generative language model一般都聚焦与低资源的language-only合成任务，但是本文的方式聚焦于合成caption在 large-scale vision-language pre-training任务重。</li>
</ul>
<h2 id="3-method">3. Method<a class="headerlink" href="#3-method" title="Permanent link">&para;</a></h2>
<h3 id="31-model-architecture">3.1 Model Architecture<a class="headerlink" href="#31-model-architecture" title="Permanent link">&para;</a></h3>
<ul>
<li>模型结构图： <img alt="" src="../pic/blip/blip1.png" /></li>
<li>Unimodal encoder：<ul>
<li>image encoder：VIT（Visual Transform）；</li>
<li>text encoder：BERT；</li>
</ul>
</li>
<li>multi-task model，多任务学习；<ul>
<li>任务1：ITC（image-text contrastive）， understanding-based objective，图片和文本描述的对比学习任务；</li>
<li>任务2：ITM（image-text matching）， understanding-based objective，图片和文本的匹配任务；</li>
<li>任务3：LM任务，generation- based objective，generate captions given images；</li>
</ul>
</li>
</ul>
<h3 id="32-pre-training-objectives">3.2 Pre-training Objectives<a class="headerlink" href="#32-pre-training-objectives" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Image-Text Contrastive Loss (ITC)</strong>：目标是将图片Visual Transformer和文本Text Transformer特征空间对齐。对齐方式：图片内容和文本内容做对比学习。</li>
<li><strong>Image-Text Matching Loss (ITM)</strong>：目标是学习image-text多模态表示学习，使得表示向量可以捕获vision和language合适粒度对齐；二分类模型：判断图文是否匹配二分类。</li>
<li><strong>Language Modeling Loss (LM)</strong>：目标是通过给定的图片生成文本；<ul>
<li>SA：self-attention </li>
<li>CA：cross-attention</li>
<li>FFN：feed forward network </li>
<li>因果关系注意力模型：causal self-attention to predict next tokens</li>
</ul>
</li>
</ul>
<h3 id="33-capfilt">3.3 CapFilt<a class="headerlink" href="#33-capfilt" title="Permanent link">&para;</a></h3>
<ul>
<li>整理框架结构： <img alt="" src="../pic/blip/blip2.png" /></li>
<li>该方法的目标：提高文本数据集的数据质量；</li>
<li><em>captioner</em>：根据给定的图片生成对应的captions；</li>
<li><em>filter</em>：剔除image-text的噪声数据；</li>
<li><em>captioner</em>和<em>filter</em>最开始是通过一些人工标注数据来初始化。The captioner and filter are initialized from the same pre-trained model and finetuned individually on a small-scale human-annotated dataset；</li>
</ul>
<h2 id="4-experiments-and-discussions">4. Experiments and Discussions<a class="headerlink" href="#4-experiments-and-discussions" title="Permanent link">&para;</a></h2>
<ul>
<li>多样性是合成caption的关键，Diversity is Key for Synthetic Captions；</li>
<li>采用nucleus sampling去生成Synthetic Captions；</li>
<li>stochastic（随机的） decoding：nucleus sampling====》可以生成多种类型并出乎意料的caption；</li>
<li>deterministic（确定性的） decoding：beam search===》确定性生成高质量或者安全的caption；</li>
<li>VLP：vision language pre-train；</li>
</ul>
<h2 id="6-additional-ablation-study">6. Additional Ablation Study<a class="headerlink" href="#6-additional-ablation-study" title="Permanent link">&para;</a></h2>
<ul>
<li>longer training并没有什么卵用；</li>
<li>continue training并没有什么卵用；</li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../LLaMa%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="btn btn-neutral float-left" title="【2】LLaMa论文阅读笔记"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../Tag2Text%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" class="btn btn-neutral float-right" title="【2】Tag2Text论文阅读笔记">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
      <p>2023, Desheng Wang</p>
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../LLaMa%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../Tag2Text%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
